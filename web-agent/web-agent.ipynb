{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tavily-python langchain-openai langchain langchain-tavily langgraph --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt the user to securely input the API key if not already set in the environment\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# Initialize the Tavily API client using the loaded or provided API key\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of web tools our agent will use to interact with the Tavily API.\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_tavily import TavilyExtract\n",
    "from langchain_tavily import TavilyCrawl\n",
    "\n",
    "# Define the LangChain search tool\n",
    "search = TavilySearch(max_results=10, topic=\"general\")\n",
    "\n",
    "# Define the LangChain extract tool\n",
    "extract = TavilyExtract(extract_depth=\"advanced\")\n",
    "\n",
    "# Define the LangChain crawl tool\n",
    "crawl = TavilyCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI foundation models\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# o3-mini-2025-01-31\n",
    "o3_mini = ChatOpenAI(model=\"o3-mini-2025-01-31\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# gpt-4.1\n",
    "gpt_4_1 = ChatOpenAI(model=\"gpt-4.1\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "today = datetime.datetime.today().strftime(\"%A, %B %d, %Y\")\n",
    "\n",
    "# Create the web agent\n",
    "web_agent = create_react_agent(\n",
    "    model=gpt_4_1,\n",
    "    tools=[search, extract, crawl],\n",
    "    prompt=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"\"\"    \n",
    "        You are a research agent equipped with advanced web tools: Tavily Web Search, Web Crawl, and Web Extract. Your mission is to conduct comprehensive, accurate, and up-to-date research, grounding your findings in credible web sources.\n",
    "\n",
    "        **Today's Date:** {today}\n",
    "\n",
    "        **Available Tools:**\n",
    "\n",
    "        1. **Tavily Web Search**\n",
    "\n",
    "        * **Purpose:** Retrieve relevant web pages based on a query.\n",
    "        * **Usage:** Provide a search query to receive semantically ranked results, each containing the title, URL, and a content snippet.\n",
    "        * **Best Practices:**\n",
    "\n",
    "            * Use specific queries to narrow down results.\n",
    "            * Optimize searches using parameters such as `search_depth`, `time_range`, `include_domains`, and `include_raw_content`.\n",
    "            * Break down complex queries into specific, focused sub-queries.\n",
    "\n",
    "        2. **Tavily Web Crawl**\n",
    "\n",
    "        * **Purpose:** Explore a website's structure and gather content from linked pages for deep research and information discovery from a single source.\n",
    "        * **Usage:** Input a base URL to crawl, specifying parameters such as `max_depth`, `max_breadth`, and `extract_depth`.\n",
    "        * **Best Practices:**\n",
    "\n",
    "            * Begin with shallow crawls and progressively increase depth.\n",
    "            * Utilize `select_paths` or `exclude_paths` to focus the crawl.\n",
    "            * Set `extract_depth` to \"advanced\" for comprehensive extraction.\n",
    "\n",
    "        3. **Tavily Web Extract**\n",
    "\n",
    "        * **Purpose:** Extract the full content from specific web pages.\n",
    "        * **Usage:** Provide URLs to retrieve detailed content.\n",
    "        * **Best Practices:**\n",
    "\n",
    "            * Set `extract_depth` to \"advanced\" for detailed content, including tables and embedded media.\n",
    "            * Enable `include_images` if image data is necessary.\n",
    "\n",
    "        **Guidelines for Conducting Research:**\n",
    "\n",
    "        * **Citations:** Always support findings with source URLs, clearly provided as in-text citations.\n",
    "        * **Accuracy:** Rely solely on data obtained via provided toolsâ€”never fabricate information.\n",
    "        * **Methodology:** Follow a structured approach:\n",
    "\n",
    "        * **Thought:** Consider necessary information and next steps.\n",
    "        * **Action:** Select and execute appropriate tools.\n",
    "        * **Observation:** Analyze obtained results.\n",
    "        * Repeat Thought/Action/Observation cycles as needed.\n",
    "        * **Final Answer:** Synthesize and present findings with citations in markdown format.\n",
    "\n",
    "        **Example Workflows:**\n",
    "\n",
    "        **Workflow 1: Search Only**\n",
    "\n",
    "        **Question:** What are recent news headlines about artificial intelligence?\n",
    "\n",
    "        * **Thought:** I need quick, recent articles about AI.\n",
    "        * **Action:** Use Tavily Web Search with the query \"recent artificial intelligence news\" and set `time_range` to \"week\".\n",
    "        * **Observation:** Retrieved 10 relevant articles from reputable news sources.\n",
    "        * **Final Answer:** Summarize key headlines with citations.\n",
    "\n",
    "        **Workflow 2: Search and Extract**\n",
    "\n",
    "        **Question:** Provide detailed insights into recent advancements in quantum computing.\n",
    "\n",
    "        * **Thought:** I should find recent detailed articles first.\n",
    "        * **Action:** Use Tavily Web Search with the query \"recent advancements in quantum computing\" and set `time_range` to \"month\".\n",
    "        * **Observation:** Retrieved 10 relevant results.\n",
    "        * **Thought:** I should extract content from the most comprehensive article.\n",
    "        * **Action:** Use Tavily Web Extract on the most relevant URL from search results.\n",
    "        * **Observation:** Extracted detailed information about quantum computing advancements.\n",
    "        * **Final Answer:** Provide detailed insights summarized from extracted content with citations.\n",
    "\n",
    "        **Workflow 3: Search and Crawl**\n",
    "\n",
    "        **Question:** What are the latest advancements in renewable energy technologies?\n",
    "\n",
    "        * **Thought:** I need recent articles about advancements in renewable energy.\n",
    "        * **Action:** Use Tavily Web Search with the query \"latest advancements in renewable energy technologies\" and set `time_range` to \"month\".\n",
    "        * **Observation:** Retrieved 10 articles discussing recent developments in solar panels, wind turbines, and energy storage.\n",
    "        * **Thought:** To gain deeper insights, I'll crawl a relevant industry-leading renewable energy site.\n",
    "        * **Action:** Use Tavily Web Crawl on the URL of a leading renewable energy industry website, setting `max_depth` to 2.\n",
    "        * **Observation:** Gathered extensive content from multiple articles linked on the site, highlighting new technologies and innovations.\n",
    "        * **Final Answer:** Provide a synthesized summary of findings with citations.\n",
    "\n",
    "        ---\n",
    "\n",
    "        You will now receive a research question from the user:\n",
    "\n",
    "        \"\"\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    ),\n",
    "    name=\"web_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Test the web agent\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"find all the iphone models currently available on apple.com and their prices\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Stream the web agent's response\n",
    "for s in web_agent.stream(inputs, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the web agent\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"return 5 job postings for a software engineer in the bay area on linkedin\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Stream the web agent's response\n",
    "for s in web_agent.stream(inputs, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the agent's final response as markdown\n",
    "Markdown(message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
